# TRU V2 VLM Inference API Configuration
# Copy this file to .env and adjust values as needed

# Model settings
MODEL_PATH=Qwen/Qwen3-VL-8B-Instruct
MODEL_BACKEND=hf  # Options: "hf" (HuggingFace) or "vllm"
USE_FLASH_ATTN=true

# vLLM settings (only used when MODEL_BACKEND=vllm)
VLLM_GPU_MEMORY_UTILIZATION=0.85
VLLM_TENSOR_PARALLEL_SIZE=1

# Video processing defaults
DEFAULT_VIDEO_FPS=2.0
DEFAULT_MAX_FRAMES=768
IMAGE_PATCH_SIZE=16

# Generation defaults
MAX_NEW_TOKENS=2048
TEMPERATURE=0.7
TOP_P=0.8
TOP_K=20

# API settings
MAX_VIDEO_SIZE_MB=1000
UPLOAD_DIR=/tmp/tru-v2-uploads

# CORS (add your frontend URL)
CORS_ORIGINS=["http://localhost:3000"]

# Logging
LOG_LEVEL=INFO
DEBUG=false

