# Core FastAPI and server
fastapi==0.109.0
uvicorn[standard]==0.27.0
pydantic==2.5.3
pydantic-settings==2.1.0
python-multipart==0.0.6
aiofiles==23.2.1

# Model and ML frameworks
torch>=2.0.0
torchvision>=0.19.0
transformers>=4.57.0
accelerate>=0.25.0

# Qwen VL utilities
qwen-vl-utils==0.0.14

# Optional: vLLM for production inference (requires CUDA)
# Uncomment if using vLLM backend
# vllm>=0.11.0

# Optional: Flash Attention for memory optimization
# Uncomment if using flash attention
# flash-attn>=2.0.0

# Optional: Better video decoding
torchcodec==0.7

# Image and video processing
pillow>=10.0.0
requests>=2.31.0
packaging>=23.0

# Logging and monitoring
python-json-logger==2.0.7

